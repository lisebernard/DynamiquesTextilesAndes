@article{cardonRevancheNeuronesInvention2018,
  title = {La revanche des neurones. L'invention des machines inductives et la controverse de l'intelligence artificielle.},
  shorttitle = {La revanche des neurones},
  author = {Cardon, Dominique and Cointet, Jean-Philippe and Mazières, Antoine},
  date = {2018},
  journaltitle = {Réseaux},
  shortjournal = {Réseaux},
  volume = {211},
  number = {5},
  pages = {173--220},
  publisher = {La Découverte},
  location = {Paris},
  issn = {0751-7971},
  doi = {10.3917/res.211.0173},
  langid = {french},
  keywords = {deep learning,intelligence artificielle}
}

@online{cohen-addadHierarchicalClusteringObjective2017,
  title = {Hierarchical {{Clustering}}: {{Objective Functions}} and {{Algorithms}}},
  shorttitle = {Hierarchical {{Clustering}}},
  author = {Cohen-Addad, Vincent and Kanade, Varun and Mallmann-Trenn, Frederik and Mathieu, Claire},
  date = {2017-04-07},
  eprint = {1704.02147},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1704.02147},
  url = {http://arxiv.org/abs/1704.02147},
  urldate = {2023-12-21},
  abstract = {Hierarchical clustering is a recursive partitioning of a dataset into clusters at an increasingly finer granularity. Motivated by the fact that most work on hierarchical clustering was based on providing algorithms, rather than optimizing a specific objective, Dasgupta framed similarity-based hierarchical clustering as a combinatorial optimization problem, where a `good' hierarchical clustering is one that minimizes some cost function. He showed that this cost function has certain desirable properties. We take an axiomatic approach to defining `good' objective functions for both similarity and dissimilarity-based hierarchical clustering. We characterize a set of "admissible" objective functions (that includes Dasgupta's one) that have the property that when the input admits a `natural' hierarchical clustering, it has an optimal value. Equipped with a suitable objective function, we analyze the performance of practical algorithms, as well as develop better algorithms. For similarity-based hierarchical clustering, Dasgupta showed that the divisive sparsest-cut approach achieves an \$O(\textbackslash log\textasciicircum\{3/2\} n)\$-approximation. We give a refined analysis of the algorithm and show that it in fact achieves an \$O(\textbackslash sqrt\{\textbackslash log n\})\$-approx. (Charikar and Chatziafratis independently proved that it is a \$O(\textbackslash sqrt\{\textbackslash log n\})\$-approx.). This improves upon the LP-based \$O(\textbackslash log n)\$-approx. of Roy and Pokutta. For dissimilarity-based hierarchical clustering, we show that the classic average-linkage algorithm gives a factor 2 approx., and provide a simple and better algorithm that gives a factor 3/2 approx.. Finally, we consider `beyond-worst-case' scenario through a generalisation of the stochastic block model for hierarchical clustering. We show that Dasgupta's cost function has desirable properties for these inputs and we provide a simple 1 + o(1)-approximation in this setting.},
  pubstate = {preprint},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning},
  file = {/Users/lise/Zotero/storage/5EJ9DPP3/1704.html}
}

@online{hintonImprovingNeuralNetworks2012,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
  date = {2012},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/lise/Zotero/storage/MK7I5FRW/Hinton et al. - 2012 - Improving neural networks by preventing co-adaptat.pdf}
}

@article{hosnaTransferLearningFriendly2022,
  title = {Transfer Learning: A Friendly Introduction},
  shorttitle = {Transfer Learning},
  author = {Hosna, Asmaul and Merry, Ethel and Gyalmo, Jigmey and Alom, Zulfikar and Aung, Zeyar and Azim, Mohammad Abdul},
  date = {2022},
  journaltitle = {Journal of Big Data},
  shortjournal = {Journal of Big Data},
  volume = {9},
  number = {1},
  pages = {102},
  issn = {2196-1115},
  doi = {10.1186/s40537-022-00652-w},
  url = {https://doi.org/10.1186/s40537-022-00652-w},
  urldate = {2023-12-21},
  abstract = {Infinite numbers of real-world applications use Machine Learning (ML) techniques to develop potentially the best data available for the users. Transfer learning (TL), one of the categories under ML, has received much attention from the research communities in the past few years. Traditional ML algorithms perform under the assumption that a model uses limited data distribution to train and test samples. These conventional methods predict target tasks undemanding and are applied to small data distribution. However, this issue conceivably is resolved using TL. TL is acknowledged for its connectivity among the additional testing and training samples resulting in faster output with efficient results. This paper contributes to the domain and scope of TL, citing situational use based on their periods and a few of its applications. The paper provides an in-depth focus on the techniques; Inductive TL, Transductive TL, Unsupervised TL, which consists of sample selection, and domain adaptation, followed by contributions and future directions.},
  keywords = {Domain adaptation,Image classification,Machine learning,Multi-task learning,Prioritaire,Sample selection,Sentiment classification,Transfer learning,Zero shot translation},
  file = {/Users/lise/Zotero/storage/PWZBW2L5/Hosna et al. - 2022 - Transfer learning a friendly introduction.pdf;/Users/lise/Zotero/storage/72MXA78P/s40537-022-00652-w.html}
}

@article{jolliffePrincipalComponentAnalysis2016,
  title = {Principal Component Analysis: A Review and Recent Developments},
  shorttitle = {Principal Component Analysis},
  author = {Jolliffe, Ian T. and Cadima, Jorge},
  date = {2016-04-13},
  journaltitle = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {374},
  number = {2065},
  pages = {20150202},
  publisher = {Royal Society},
  doi = {10.1098/rsta.2015.0202},
  url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202},
  urldate = {2023-12-21},
  abstract = {Large datasets are increasingly common and are often difficult to interpret. Principal component analysis (PCA) is a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss. It does so by creating new uncorrelated variables that successively maximize variance. Finding such new variables, the principal components, reduces to solving an eigenvalue/eigenvector problem, and the new variables are defined by the dataset at hand, not a priori, hence making PCA an adaptive data analysis technique. It is adaptive in another sense too, since variants of the technique have been developed that are tailored to various different data types and structures. This article will begin by introducing the basic ideas of PCA, discussing what it can and cannot do. It will then describe some variants of PCA and their application.},
  keywords = {dimension reduction,eigenvectors,multivariate analysis,palaeontology},
  file = {/Users/lise/Zotero/storage/6AEXZQHQ/Jolliffe et Cadima - 2016 - Principal component analysis a review and recent .pdf}
}

@article{krizhevskyImageNetClassificationDeep2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  date = {2017-05-24},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3065386},
  url = {https://dl.acm.org/doi/10.1145/3065386},
  urldate = {2024-05-23},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  langid = {english},
  file = {/Users/lise/Zotero/storage/F48MT6XD/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf}
}

@book{krizhevskyLearningMultipleLayers2009,
  title = {Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}},
  author = {Krizhevsky, A.},
  date = {2009},
  publisher = {University of Toronto},
  location = {Toronto},
  url = {https://www.semanticscholar.org/paper/Learning-Multiple-Layers-of-Features-from-Tiny-Krizhevsky/5d90f06bb70a0a3dced62413346235c02b1aa086},
  urldate = {2023-06-11},
  abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly improved by pre-training a layer of features on a large set of unlabeled tiny images.},
  file = {/Users/lise/Zotero/storage/3HYLRBK7/Krizhevsky - 2009 - Learning Multiple Layers of Features from Tiny Ima.pdf}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  date = {2015},
  journaltitle = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  doi = {10.1038/nature14539},
  langid = {english},
  file = {/Users/lise/Zotero/storage/T6KVJ5C3/LeCun et al. - 2015 - Deep learning.pdf}
}

@inproceedings{malzerHybridApproachHierarchical2020,
  title = {A {{Hybrid Approach To Hierarchical Density-based Cluster Selection}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Multisensor Fusion}} and {{Integration}} for {{Intelligent Systems}} ({{MFI}})},
  author = {Malzer, Claudia and Baum, Marcus},
  date = {2020-09-14},
  eprint = {1911.02282},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {223--228},
  doi = {10.1109/MFI49285.2020.9235263},
  url = {http://arxiv.org/abs/1911.02282},
  urldate = {2023-12-21},
  abstract = {HDBSCAN is a density-based clustering algorithm that constructs a cluster hierarchy tree and then uses a specific stability measure to extract flat clusters from the tree. We show how the application of an additional threshold value can result in a combination of DBSCAN* and HDBSCAN clusters, and demonstrate potential benefits of this hybrid approach when clustering data of variable densities. In particular, our approach is useful in scenarios where we require a low minimum cluster size but want to avoid an abundance of micro-clusters in high-density regions. The method can directly be applied to HDBSCAN's tree of cluster candidates and does not require any modifications to the hierarchy itself. It can easily be integrated as an addition to existing HDBSCAN implementations.},
  keywords = {Computer Science - Databases},
  file = {/Users/lise/Zotero/storage/NVQKLAHZ/Malzer et Baum - 2020 - A Hybrid Approach To Hierarchical Density-based Cl.pdf;/Users/lise/Zotero/storage/DWCG6QNX/1911.html}
}

@article{mccullochLogicalCalculusIdeas1943,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {McCulloch, Warren S. and Pitts, Walter},
  date = {1943},
  journaltitle = {The bulletin of mathematical biophysics},
  shortjournal = {Bulletin of Mathematical Biophysics},
  volume = {5},
  number = {4},
  pages = {115--133},
  issn = {1522-9602},
  doi = {10.1007/BF02478259},
  url = {https://doi.org/10.1007/BF02478259},
  urldate = {2023-06-11},
  abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
  langid = {english}
}

@article{mcinnesHdbscanHierarchicalDensity2017,
  title = {Hdbscan: {{Hierarchical}} Density Based Clustering},
  shorttitle = {Hdbscan},
  author = {McInnes, Leland and Healy, John and Astels, Steve},
  date = {2017},
  journaltitle = {Journal of Open Source Software},
  volume = {2},
  number = {11},
  pages = {205},
  doi = {10.21105/joss.00205},
  langid = {english},
  file = {/Users/lise/Zotero/storage/WKDXX2L6/McInnes et al. - 2017 - hdbscan Hierarchical density based clustering.pdf}
}

@article{mcinnesUMAPUniformManifold2018,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  date = {2018},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1802.03426},
  url = {https://arxiv.org/abs/1802.03426},
  urldate = {2023-02-23},
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  version = {3},
  keywords = {Important,machine learning}
}

@online{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date = {2016},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/lise/Zotero/storage/EQUKS2ED/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf}
}

@online{sanghviImageClassificationTechniques2023,
  title = {Image {{Classification Techniques}}},
  author = {Sanghvi, Kavish},
  date = {2023-04-21T22:45:57},
  url = {https://medium.com/analytics-vidhya/image-classification-techniques-83fd87011cac},
  urldate = {2023-12-21},
  abstract = {Image classification refers to a process in computer vision that can classify an image according to its visual content.},
  langid = {english},
  organization = {Analytics Vidhya}
}

@online{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2015-04-10},
  eprint = {1409.1556},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1409.1556},
  url = {http://arxiv.org/abs/1409.1556},
  urldate = {2023-12-21},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/lise/Zotero/storage/4RVS9H4K/Simonyan et Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf;/Users/lise/Zotero/storage/H9KZ47IT/1409.html}
}
